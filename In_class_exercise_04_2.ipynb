{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kamalrajugambli/KamalRaju_INFO5731_SPRING2022/blob/main/In_class_exercise_04_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsjwXaB0skrC"
      },
      "source": [
        "# **The fourth in-class-exercise (40 points in total, 03/29/2022)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH65mkmVskrF"
      },
      "source": [
        "Question description: Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2clk55nYskrG"
      },
      "source": [
        "## (1) (10 points) Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here: \n",
        "\n",
        "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kT_7uTliskrG",
        "outputId": "1104b683-fee9-4269-d3ee-ecd164fcc09b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyldavis\n",
            "  Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
            "\u001b[?25l\r\u001b[K     |▏                               | 10 kB 34.0 MB/s eta 0:00:01\r\u001b[K     |▍                               | 20 kB 42.6 MB/s eta 0:00:01\r\u001b[K     |▋                               | 30 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |▉                               | 40 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |█                               | 51 kB 12.7 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 61 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 71 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 81 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 92 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |██                              | 102 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 112 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 122 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 133 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 143 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |███                             | 153 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 163 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 174 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 184 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 194 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |████                            | 204 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 215 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 225 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 235 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 245 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |█████                           | 256 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 266 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 276 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 286 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 296 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |██████                          | 307 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 317 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 327 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 337 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 348 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |███████                         | 358 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 368 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 378 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 389 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 399 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |████████                        | 409 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 419 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 430 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 440 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 450 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 460 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 471 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 481 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 491 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 501 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 512 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 522 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 532 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 542 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 552 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 563 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 573 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 583 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 593 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 604 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 614 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 624 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 634 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 645 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 655 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 665 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 675 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 686 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 696 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 706 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 716 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 727 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 737 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 747 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 757 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 768 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 778 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 788 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 798 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 808 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 819 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 829 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 839 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 849 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 860 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 870 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 880 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 890 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 901 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 911 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 921 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 931 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 942 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 952 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 962 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 972 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 983 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 993 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.0 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.0 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.0 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.0 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.0 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.1 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.1 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.1 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.1 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.1 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.1 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.1 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.1 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.1 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.1 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.2 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.2 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.2 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.2 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.2 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.2 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.2 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.2 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.2 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.2 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.3 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.3 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.3 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.3 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.3 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.3 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.3 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.3 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.3 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.4 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.4 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.4 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.4 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.4 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.4 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.4 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.4 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.4 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.4 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.5 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.5 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.5 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.5 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.5 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.5 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.5 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.5 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.5 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.5 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.6 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.6 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.6 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.6 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.6 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.6 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.6 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.6 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.6 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.6 MB 13.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.7 MB 13.8 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyldavis) (1.4.1)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from pyldavis) (1.3.5)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from pyldavis) (3.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from pyldavis) (2.11.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyldavis) (0.16.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from pyldavis) (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyldavis) (1.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pyldavis) (57.4.0)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyldavis) (2.8.1)\n",
            "Collecting funcy\n",
            "  Downloading funcy-1.17-py2.py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.7/dist-packages (from pyldavis) (1.21.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyldavis) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyldavis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyldavis) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyldavis) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->pyldavis) (5.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->pyldavis) (2.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from numexpr->pyldavis) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->numexpr->pyldavis) (3.0.7)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyldavis) (3.1.0)\n",
            "Building wheels for collected packages: pyldavis\n",
            "  Building wheel for pyldavis (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyldavis: filename=pyLDAvis-3.3.1-py2.py3-none-any.whl size=136898 sha256=0a01e59baef711c0c7484485fdf343d7ba8f4fe6ecce9d28b2b5349a2d0cc6bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/21/f6/17bcf2667e8a68532ba2fbf6d5c72fdf4c7f7d9abfa4852d2f\n",
            "Successfully built pyldavis\n",
            "Installing collected packages: funcy, pyldavis\n",
            "Successfully installed funcy-1.17 pyldavis-3.3.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/past/types/oldstr.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  from collections import Iterable\n"
          ]
        }
      ],
      "source": [
        "#importing required packages\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "import spacy\n",
        "\n",
        "!pip install pyldavis\n",
        "import pyLDAvis\n",
        "import gensim\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data1=pd.read_csv(\"/content/cleandata_list.csv\")\n",
        "data = data1.Content.values.tolist()\n",
        "# Remove Emails\n",
        "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
        "\n",
        "# Remove new line characters\n",
        "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
        "\n",
        "# Remove distracting single quotes\n",
        "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
        "\n",
        "#Old Column data\n",
        "print(data1[\"Content\"][1])\n",
        "\n",
        "#New column data\n",
        "print(data[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMUuoR586eQy",
        "outputId": "08e4bfda-0d57-47bf-f256-38dbd3b919b0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"... The concept of maximum entropy can be traced back along multiple threads to/ nitish@gmail.com  Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper we des ...\"\n",
            "\"... The concept of maximum entropy can be traced back along multiple threads to/ Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper we des ...\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  \n",
        "\n",
        "data_words = list(sent_to_words(data))\n",
        "\n",
        "print(data_words[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAJjLQH3PEid",
        "outputId": "92b8f8d6-394d-465f-b7f4-a16dcd0c206d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'concept', 'of', 'maximum', 'entropy', 'can', 'be', 'traced', 'back', 'along', 'multiple', 'threads', 'to', 'biblical', 'times', 'only', 'recently', 'however', 'have', 'computers', 'become', 'powerful', 'enough', 'to', 'permit', 'the', 'widescale', 'application', 'of', 'this', 'concept', 'to', 'real', 'world', 'problems', 'in', 'statistical', 'estimation', 'and', 'pattern', 'recognition', 'in', 'this', 'paper', 'we', 'des']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the bigram and trigram models\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
        "\n",
        "# Faster way to get a sentence clubbed as a trigram/bigram\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "\n",
        "# See trigram example\n",
        "print(trigram_mod[bigram_mod[data_words[1]]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RYaB-s6PikS",
        "outputId": "9435f294-e6f1-4a82-c689-17b40da16c5c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'concept', 'of', 'maximum', 'entropy', 'can', 'be', 'traced', 'back', 'along', 'multiple', 'threads', 'to', 'biblical', 'times', 'only', 'recently', 'however', 'have', 'computers', 'become', 'powerful', 'enough', 'to', 'permit', 'the', 'widescale', 'application', 'of', 'this', 'concept', 'to', 'real', 'world', 'problems', 'in', 'statistical', 'estimation', 'and', 'pattern', 'recognition', 'in', 'this', 'paper', 'we', 'des']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def make_trigrams(texts):\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out"
      ],
      "metadata": {
        "id": "8p9lmw6uP3tL"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stop Words\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "# Form Bigrams\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
        "# python3 -m spacy download en\n",
        "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "\n",
        "# Do lemmatization keeping only noun, adj, vb, adv\n",
        "Clean_data = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "print(Clean_data[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "VrRK97oJP7eP",
        "outputId": "89c1e672-c30c-4228-fd8b-0584fc72bc1f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-35744888d504>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Remove Stop Words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_words_nostops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Form Bigrams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata_words_bigrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_bigrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_words_nostops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-a9dac53f8cd2>\u001b[0m in \u001b[0;36mremove_stopwords\u001b[0;34m(texts)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Define functions for stopwords, bigrams, trigrams and lemmatization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msimple_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_bigrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-a9dac53f8cd2>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Define functions for stopwords, bigrams, trigrams and lemmatization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msimple_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_bigrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-a9dac53f8cd2>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Define functions for stopwords, bigrams, trigrams and lemmatization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msimple_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_bigrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'stop_words' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(Clean_data)\n",
        "\n",
        "# Create Corpus\n",
        "texts = Clean_data\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "# View\n",
        "print(corpus[1])"
      ],
      "metadata": {
        "id": "XXOkDiEjQByy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2word[0]"
      ],
      "metadata": {
        "id": "4Nyd0irMQJGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Human readable format of corpus (term-frequency)\n",
        "[[(id2word[id], freq) for id, freq in cp] for cp in corpus]"
      ],
      "metadata": {
        "id": "VXjKNl8vQOLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build LDA model\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=20, \n",
        "                                           random_state=100,\n",
        "                                           update_every=1,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           alpha='auto',\n",
        "                                           per_word_topics=True)"
      ],
      "metadata": {
        "id": "uJChpQUNV2-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the Keyword in the 10 topics\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ],
      "metadata": {
        "id": "uNqTXqQUV5VY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  \n",
        "\n",
        "\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=Clean_data, dictionary=id2word, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)"
      ],
      "metadata": {
        "id": "UsAauYerWAcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlj740l5skrH"
      },
      "source": [
        "## (2) (10 points) Generate K topics by using LSA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import modules\n",
        "import os.path\n",
        "from gensim import corpora\n",
        "from gensim.models import LsiModel\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "7kJMfI4AP5zR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jadCq3j0skrI"
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_data(path,file_name):\n",
        "    \"\"\"\n",
        "    Input  : path and file_name\n",
        "    Purpose: loading text file\n",
        "    Output : list of paragraphs/documents and\n",
        "             title(initial 100 words considred as title of document)\n",
        "    \"\"\"\n",
        "    documents_list = []\n",
        "    titles=[]\n",
        "    with open( os.path.join(path, file_name) ,\"r\") as fin:\n",
        "        for line in fin.readlines():\n",
        "            text = line.strip()\n",
        "            documents_list.append(text)\n",
        "    print(\"Total Number of Documents:\",len(documents_list))\n",
        "    titles.append( text[0:min(len(text),100)] )\n",
        "    return documents_list,titles\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(doc_set):\n",
        "    \"\"\"\n",
        "    Input  : docuemnt list\n",
        "    Purpose: preprocess text (tokenize, removing stopwords, and stemming)\n",
        "    Output : preprocessed text\n",
        "    \"\"\"\n",
        "    # initialize regex tokenizer\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    # create English stop words list\n",
        "    en_stop = set(stopwords.words('english'))\n",
        "    # Create p_stemmer of class PorterStemmer\n",
        "    p_stemmer = PorterStemmer()\n",
        "    # list for tokenized documents in loop\n",
        "    texts = []\n",
        "    # loop through document list\n",
        "    for i in doc_set:\n",
        "        # clean and tokenize document string\n",
        "        raw = i.lower()\n",
        "        tokens = tokenizer.tokenize(raw)\n",
        "        # remove stop words from tokens\n",
        "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
        "        # stem tokens\n",
        "        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
        "        # add tokens to list\n",
        "        texts.append(stemmed_tokens)\n",
        "    return texts\n",
        "\n",
        "def prepare_corpus(doc_clean):\n",
        "    \"\"\"\n",
        "    Input  : clean document\n",
        "    Purpose: create term dictionary of our courpus and Converting list of documents (corpus) into Document Term Matrix\n",
        "    Output : term dictionary and Document Term Matrix\n",
        "    \"\"\"\n",
        "    # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
        "    dictionary = corpora.Dictionary(doc_clean)\n",
        "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
        "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
        "    # generate LDA model\n",
        "    return dictionary,doc_term_matrix\n"
      ],
      "metadata": {
        "id": "YmkIkhiTjw8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_gensim_lsa_model(doc_clean,number_of_topics,words):\n",
        "    \"\"\"\n",
        "    Input  : clean document, number of topics and number of words associated with each topic\n",
        "    Purpose: create LSA model using gensim\n",
        "    Output : return LSA model\n",
        "    \"\"\"\n",
        "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
        "    # generate LSA model\n",
        "    lsamodel = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
        "    print(lsamodel.print_topics(num_topics=number_of_topics, num_words=words))\n",
        "    return lsamodel\n",
        "\n",
        "def compute_coherence_values(dictionary, doc_term_matrix, doc_clean, stop, start=2, step=3):\n",
        "    \"\"\"\n",
        "    Input   : dictionary : Gensim dictionary\n",
        "              corpus : Gensim corpus\n",
        "              texts : List of input texts\n",
        "              stop : Max num of topics\n",
        "    purpose : Compute c_v coherence for various number of topics\n",
        "    Output  : model_list : List of LSA topic models\n",
        "              coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
        "    \"\"\"\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, stop, step):\n",
        "        # generate LSA model\n",
        "        model = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "    return model_list, coherence_values"
      ],
      "metadata": {
        "id": "uoHLGOeej8mF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graph(doc_clean,start, stop, step):\n",
        "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
        "    model_list, coherence_values = compute_coherence_values(dictionary, doc_term_matrix,doc_clean,\n",
        "                                                            stop, start, step)\n",
        "    # Show graph\n",
        "    x = range(start, stop, step)\n",
        "    plt.plot(x, coherence_values)\n",
        "    plt.xlabel(\"Number of Topics\")\n",
        "    plt.ylabel(\"Coherence score\")\n",
        "    plt.legend((\"coherence_values\"), loc='best')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "oaXs354pkERL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start,stop,step=2,12,1\n",
        "plot_graph(Clean_data,start,stop,step)"
      ],
      "metadata": {
        "id": "AK-34cq9kHWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LSA Model\n",
        "number_of_topics=6\n",
        "words=10\n",
        "model=create_gensim_lsa_model(Clean_data,number_of_topics,words)"
      ],
      "metadata": {
        "id": "3kTOncuUm666"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary,doc_term_matrix= prepare_corpus(Clean_data)\n",
        "model_list_lsa,coherence_values_lsa = compute_coherence_values(dictionary,doc_term_matrix,Clean_data,40,2,6)"
      ],
      "metadata": {
        "id": "AFLQ74PYq7YP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = range(start, stop, step)\n",
        "for m, cv in zip(x, coherence_values_lsa):\n",
        "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
      ],
      "metadata": {
        "id": "GNa0pfPBq9qA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tPotsahskrI"
      },
      "source": [
        "## (3) (10 points) Generate K topics by using  lda2vec, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://nbviewer.org/github/cemoody/lda2vec/blob/master/examples/twenty_newsgroups/lda2vec/lda2vec.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "_XC0HYA2skrJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "259c71d8-75f5-4c62-a25e-f1cf099a153a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-9c006827cf37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Write your code here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlda2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lda2vec'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Write your code here\n",
        "from lda2vec import preprocess, Corpus\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "try:\n",
        "    import seaborn\n",
        "except:\n",
        "    pass\n",
        "\n",
        "\n",
        "import pyLDAvis\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "\n",
        "npz = np.load(open('topics.pyldavis.npz', 'r'))\n",
        "dat = {k: v for (k, v) in npz.iteritems()}\n",
        "dat['vocab'] = dat['vocab'].tolist()\n",
        "# dat['term_frequency'] = dat['term_frequency'] * 1.0 / dat['term_frequency'].sum()\n",
        "top_n = 10\n",
        "topic_to_topwords = {}\n",
        "for j, topic_to_word in enumerate(dat['topic_term_dists']):\n",
        "    top = np.argsort(topic_to_word)[::-1][:top_n]\n",
        "    msg = 'Topic %i '  % j\n",
        "    top_words = [dat['vocab'][i].strip()[:35] for i in top]\n",
        "    msg += ' '.join(top_words)\n",
        "    print msg\n",
        "    topic_to_topwords[j] = top_words\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "prepared_data = pyLDAvis.prepare(dat['topic_term_dists'], dat['doc_topic_dists'], \n",
        "                                 dat['doc_lengths'] * 1.0, dat['vocab'], dat['term_frequency'] * 1.0, mds='tsne')\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "remove=('headers', 'footers', 'quotes')\n",
        "texts = fetch_20newsgroups(subset='train', remove=remove).data\n",
        "\n",
        "msg = \"{weight:02d}% in topic {topic_id:02d} which has top words {text:s}\"\n",
        "for topic_id, weight in enumerate(dat['doc_topic_dists'][1]):\n",
        "    if weight > 0.01:\n",
        "        text = ', '.join(topic_to_topwords[topic_id])\n",
        "        print msg.format(topic_id=topic_id, weight=int(weight * 100.0), text=text)\n",
        "\n",
        "\n",
        "plt.bar(np.arange(20), dat['doc_topic_dists'][1])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kusOnKlgskrK"
      },
      "source": [
        "## (4) (10 points) Generate K topics by using BERTopic, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here: \n",
        "\n",
        "https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "DasQuGimskrL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "3573cb4d-d6d2-4169-e00a-ab3c00f639d3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-4138b732ebde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_20newsgroups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mremove\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'headers'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'footers'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'quotes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbertopic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBERTopic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtopic_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBERTopic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalculate_probabilities\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bertopic'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "\n",
        "# Write your code here\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']\n",
        "\n",
        "from bertopic import BERTopic\n",
        "\n",
        "topic_model = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True)\n",
        "topics, probs = topic_model.fit_transform(docs)\n",
        "freq = topic_model.get_topic_info(); freq.head(5)\n",
        "freq = topic_model.get_topic_info(); freq.head(5)\n",
        "\n",
        "topic_model.get_topic(0)  # Select the most frequent topic\n",
        "topic_model.visualize_topics()\n",
        "topic_model.visualize_term_rank()\n",
        "opic_model.update_topics(docs, topics, n_gram_range=(1, 2))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8OFdD0HskrM"
      },
      "source": [
        "## (5) (10 extra points) Compare the results generated by the four topic modeling algorithms, which one is better? You should explain the reasons in details."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LDA (Latent Dirichlet Allocation) is a widely used topic modeling technique. It organizes text into relevant categories.\n",
        "By calculating the likelihood of finding a word in a text and comparing it to the likelihood of finding a term in a comparable topic\n",
        "\n",
        "After that, it assigns the topic. We can observe that the best number of topics is 18 using Gensim's LDA.\n",
        "\n",
        "The LDA approach yielded a coherence score of 0.77.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "LSA is a legal term that refers to (Latent Semantic Analysis)\n",
        "\n",
        "Another important model for subject modeling. It decides the subjects by transforming the text into a single value equation with the help of SVD, and then analyzing the results.\n",
        "\n",
        "All of the topics' cosine similarity is found. The main topic is chosen from among the topics with the highest cosine similarity.\n",
        "\n",
        "We can see from the preceding data that the Coherence Value is 0.43 and the number of topics is 2.\n",
        "\n",
        "\n",
        "\n",
        "When we compare the two results, we can observe that LDA has a greater coherence score than LSA.\n",
        "\n",
        "LDA fits the text well for topic modeling since a greater coherence score indicates better alignment towards a theme."
      ],
      "metadata": {
        "id": "JyMS8sgAt0TX"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "In_class_exercise_04-2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}